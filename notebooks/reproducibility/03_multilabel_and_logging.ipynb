{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6d44de-393f-4815-aabd-80d4eba94c7a",
   "metadata": {},
   "source": [
    "Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2d02f2-4d9b-498e-a134-e9f645db6ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ensure repo root on path\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "# Paths\n",
    "processed_dir = os.path.join(repo_root, 'data', 'processed')\n",
    "tfidf_path    = os.path.join(processed_dir, 'tfidf.pkl')\n",
    "train_csv     = os.path.join(processed_dir, 'train.csv')\n",
    "val_csv       = os.path.join(processed_dir, 'val.csv')\n",
    "\n",
    "# Labels\n",
    "LABELS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7c8c75-f3b5-462d-a533-cc5111964788",
   "metadata": {},
   "source": [
    "Load Data & Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce2d02b-3d4e-4138-8fa5-079c7becd92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and TF-IDF loaded:\n",
      "  • Train shape: (127656,) (127656, 6)\n",
      "  • Val   shape: (31915,) (31915, 6)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_csv)\n",
    "val_df   = pd.read_csv(val_csv)\n",
    "\n",
    "X_train = train_df['comment_text']\n",
    "y_train = train_df[LABELS]\n",
    "X_val   = val_df  ['comment_text']\n",
    "y_val   = val_df  [LABELS]\n",
    "\n",
    "with open(tfidf_path, 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "print(\"Data and TF-IDF loaded:\")\n",
    "print(\"  • Train shape:\", X_train.shape, y_train.shape)\n",
    "print(\"  • Val   shape:\", X_val.shape,   y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d092b2-fe61-4467-bde0-2a974542eaba",
   "metadata": {},
   "source": [
    "Define & Train One-vs-Rest Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046340f2-7eb3-46a8-afee-30aa9ace27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label logistic regression trained.\n",
      "\n",
      "Classification report on validation set:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.60      0.85      0.70      3059\n",
      " severe_toxic       0.24      0.88      0.38       311\n",
      "      obscene       0.63      0.88      0.73      1710\n",
      "       threat       0.16      0.70      0.26        97\n",
      "       insult       0.50      0.87      0.63      1590\n",
      "identity_hate       0.18      0.81      0.30       289\n",
      "\n",
      "    micro avg       0.49      0.86      0.63      7056\n",
      "    macro avg       0.39      0.83      0.50      7056\n",
      " weighted avg       0.54      0.86      0.66      7056\n",
      "  samples avg       0.06      0.08      0.06      7056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define base pipeline\n",
    "base_pipe = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        LogisticRegression(\n",
    "            C=1.0,\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train\n",
    "base_pipe.fit(X_train, y_train)\n",
    "print(\"Multi-label logistic regression trained.\")\n",
    "print(\"\\nClassification report on validation set:\\n\")\n",
    "print(classification_report(y_val, base_pipe.predict(X_val), target_names=LABELS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae7198-9c40-4a65-9b2c-67f74e0c3fcb",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906e59a-f511-48d3-b5a1-31ef792040c6",
   "metadata": {},
   "source": [
    "Quick Grid Search on C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ed4c61-29cb-48af-a98a-fcc264f6a8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best parameters found: {'clf__estimator__C': 10.0}\n",
      "Best CV f1_macro: 0.5135963508061614\n",
      "\n",
      "Classification report (tuned) on validation set:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.58      0.84      0.69      3059\n",
      " severe_toxic       0.25      0.79      0.38       311\n",
      "      obscene       0.60      0.90      0.72      1710\n",
      "       threat       0.23      0.60      0.33        97\n",
      "       insult       0.49      0.83      0.62      1590\n",
      "identity_hate       0.21      0.74      0.33       289\n",
      "\n",
      "    micro avg       0.50      0.84      0.63      7056\n",
      "    macro avg       0.39      0.78      0.51      7056\n",
      " weighted avg       0.53      0.84      0.65      7056\n",
      "  samples avg       0.06      0.08      0.07      7056\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ual-laptop\\anaconda3\\envs\\toxic_bias_audit\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'clf__estimator__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=base_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best parameters found:\", grid.best_params_)\n",
    "print(\"Best CV f1_macro:\", grid.best_score_)\n",
    "\n",
    "# Re-evaluate on validation set\n",
    "y_pred_tuned = grid.predict(X_val)\n",
    "print(\"\\nClassification report (tuned) on validation set:\\n\")\n",
    "print(classification_report(y_val, y_pred_tuned, target_names=LABELS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede70208-ab30-4706-a5dd-ed8732748b95",
   "metadata": {},
   "source": [
    "Save the Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3530a81c-cfa0-4ec5-8b51-ef0a31746285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned multi-label model saved to: C:\\Users\\ual-laptop\\Toxic_Bias_Audit\\experiments\\logreg_multilabel\\logreg_multilabel_tuned.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "model_dir = os.path.join(repo_root, 'experiments', 'logreg_multilabel')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'logreg_multilabel_tuned.pkl')\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(\"Tuned multi-label model saved to:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9a29d-deaf-4951-8b77-f9083b83c08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxic_bias_audit)",
   "language": "python",
   "name": "toxic_bias_audit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
