{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f48926a-b3a9-4c2c-8b89-9175d68abcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: C:\\Users\\ual-laptop\\Toxic_Bias_Audit Contents: ['.git', '.virtual_documents', 'anaconda_projects', 'data', 'Dockerfile', 'environment.yml', 'ethical_audit', 'experiments', 'notebooks', 'README.md', 'report', 'src', 'tests']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Go up two levels from notebooks/reproducibility to the repo root\n",
    "repo_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.insert(0, repo_root)\n",
    "\n",
    "print(\"Repo root:\", repo_root, \"Contents:\", os.listdir(repo_root))\n",
    "\n",
    "import pandas as pd\n",
    "from src.preprocessing import load_data, split_and_save, fit_tfidf, save_vectorizer\n",
    "\n",
    "LABELS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51099dc4-825b-452f-a5e0-0e2ab32b1e54",
   "metadata": {},
   "source": [
    "Load & Inspect Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99799664-b2c1-4e0a-8094-c29c19bd8d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples after dropping NAs: 159571\n",
      "Label counts:\n",
      "toxic            15294\n",
      "obscene           8449\n",
      "insult            7877\n",
      "severe_toxic      1595\n",
      "identity_hate     1405\n",
      "threat             478\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and clean raw data\n",
    "raw_csv = os.path.join(repo_root, 'data', 'raw', 'train.csv')\n",
    "df = load_data(raw_csv)\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"Total samples after dropping NAs: {df.shape[0]}\")\n",
    "print(\"Label counts:\")\n",
    "print(df[LABELS].sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b6c2c-e033-4278-b38b-b97f7c1ea637",
   "metadata": {},
   "source": [
    "Train/Validation Split & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a1697fd-0777-45e4-ac63-1411db612fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/validation CSVs written to C:\\Users\\ual-laptop\\Toxic_Bias_Audit\\data\\processed\n",
      "Loaded train → (127656, 7), val → (31915, 7)\n"
     ]
    }
   ],
   "source": [
    "# split and save to disk\n",
    "processed_dir = os.path.join(repo_root, 'data', 'processed')\n",
    "\n",
    "# This writes the files but does not return values\n",
    "split_and_save(\n",
    "    df,\n",
    "    labels=LABELS,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    output_dir=processed_dir\n",
    ")\n",
    "\n",
    "print(f\"Train/validation CSVs written to {processed_dir}\")\n",
    "\n",
    "# Now load them back into DataFrames\n",
    "train_df = pd.read_csv(os.path.join(processed_dir, 'train.csv'))\n",
    "val_df   = pd.read_csv(os.path.join(processed_dir, 'val.csv'))\n",
    "\n",
    "print(f\"Loaded train → {train_df.shape}, val → {val_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471616b-dff4-4a77-b918-758ea9942a31",
   "metadata": {},
   "source": [
    "Fit & Persist TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647c250c-b54a-4517-b3b5-386b1db606fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF vectorizer saved to C:\\Users\\ual-laptop\\Toxic_Bias_Audit\\data\\processed\\tfidf.pkl\n"
     ]
    }
   ],
   "source": [
    "# Fit TF-IDF on training texts only\n",
    "tfidf = fit_tfidf(\n",
    "    train_df['comment_text'],\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Save the fitted vectorizer\n",
    "tfidf_path = os.path.join(processed_dir, 'tfidf.pkl')\n",
    "save_vectorizer(tfidf, output_path=tfidf_path)\n",
    "\n",
    "print(f\"TF-IDF vectorizer saved to {tfidf_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f06c8-5409-4884-93cb-b1177b1e660d",
   "metadata": {},
   "source": [
    "Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650c6ed5-37ed-418a-9dac-3237ce52de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train shape: (127656, 7)\n",
      "Processed val shape:   (31915, 7)\n",
      "TF-IDF vocab size: 10000\n",
      "Sample features: ['unfounded', 'personal', 'attacks', 'talk', 'page', 'just', 'gets', 'better', 'suppose', 'blame']\n"
     ]
    }
   ],
   "source": [
    "# Reload processed CSVs\n",
    "train_check = pd.read_csv(os.path.join(processed_dir, 'train.csv'))\n",
    "val_check   = pd.read_csv(os.path.join(processed_dir, 'val.csv'))\n",
    "print(\"Processed train shape:\", train_check.shape)\n",
    "print(\"Processed val shape:  \", val_check.shape)\n",
    "\n",
    "# Load and inspect the vectorizer\n",
    "import pickle\n",
    "with open(os.path.join(processed_dir, 'tfidf.pkl'), 'rb') as f:\n",
    "    vec = pickle.load(f)\n",
    "print(\"TF-IDF vocab size:\", len(vec.vocabulary_))\n",
    "# Optionally show the first 10 feature names\n",
    "print(\"Sample features:\", list(vec.vocabulary_.keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaa739-fcd9-46b7-9fce-6a3d6f2477a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxic_bias_audit)",
   "language": "python",
   "name": "toxic_bias_audit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
