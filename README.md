# ğŸ”¥ Toxic Comment Detection with Bias Audit ğŸ”
 
## ğŸŒ Project Overview

**Toxic Comment Detection with Bias Audit**  
This repository implements and evaluates multiple lightweight and advanced pipelines to detect toxic comments on the Jigsaw dataset. We compare:

- **TF-IDF (word n-grams)** + Logistic Regression  
- **TF-IDF (character n-grams)** + Logistic Regression  
- **Frozen transformer embeddings** (DistilBERT encoder) + Logistic Regression  

We also discuss ethical considerations and outline a path for a comprehensive bias audit once demographic data become available.

---

## ğŸ“ Repository Structure

```text
Toxic_Bias_Audit/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/           # Original Jigsaw CSVs
â”‚   â””â”€â”€ processed/     # Cleaned & split train/val and tfidf.pkl
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprocessing.py  # Data-loading & cleaning utilities
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory/       # EDA and visualization
â”‚   â””â”€â”€ reproducibility/   # Step-by-step pipelines:
â”‚       â”œâ”€â”€ 01_preprocessing.ipynb
â”‚       â”œâ”€â”€ 02_baseline_logreg.ipynb
â”‚       â”œâ”€â”€ 03_multilabel_baseline.ipynb
â”‚       â””â”€â”€ 05_char_ngram_baseline.ipynb
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ logreg/            # Single-label artifacts
â”‚   â””â”€â”€ logreg_multilabel/ # Multi-label artifacts
â”œâ”€â”€ ethical_audit/
â”‚   â””â”€â”€ 01_fairness_audit.ipynb  # Audit notebook (no sensitive features)
â”œâ”€â”€ environment.yml       # Conda environment specification
â””â”€â”€ README.md             # â† you are here
